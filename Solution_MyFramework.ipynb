{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('pythonProject': conda)"
  },
  "interpreter": {
   "hash": "b5ed7aac63ba26b6cc13f3466962b87b151feb8ba9e841efa9abe3bad9164d9f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Лабораторная работа по курсу \"Искусственный интеллект\"\r\n",
    "## Многослойный персептрон\r\n",
    "\r\n",
    "| Студент | Иоффе |\r\n",
    "|---------|--------|\r\n",
    "| Группа  | М8О-114М-21      |\r\n",
    "\r\n",
    "Для начала, скачаем датасет MNIST. Используйте `wget` или `curl`, либо скачайте вручную [по ссылке](https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/Data/MNIST/mnist.pkl.gz)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#!wget https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/Data/MNIST/mnist.pkl.gz\r\n",
    "!curl -o mnist.pkl.gz https://raw.githubusercontent.com/shwars/NeuroWorkshop/master/Data/MNIST/mnist.pkl.gz\r\n",
    "!gzip -d mnist.pkl.gz"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0  9.9M    0 15081    0     0  58793      0  0:02:56 --:--:--  0:02:56 58910\n",
      "100  9.9M  100  9.9M    0     0  14.5M      0 --:--:-- --:--:-- --:--:-- 14.5M\n",
      "\"gzip\" �� ���� ����७��� ��� ���譥�\n",
      "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь загружаем датасет:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pickle\r\n",
    "with open('mnist.pkl','rb') as f:\r\n",
    "    MNIST = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "labels = MNIST['Train']['Labels']\r\n",
    "data = MNIST['Train']['Features']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Смотрим на то, какие данные получились:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "data.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Используйте Scikit Learn для разбиения данных на обучающую и тестовую выборку"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import numpy as np\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.33, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "data_train.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(28140, 784)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class Linear:\r\n",
    "    def __init__(self,nin,nout):\r\n",
    "        self.W = np.random.normal(0, 1.0/np.sqrt(nin), (nout, nin))\r\n",
    "        self.b = np.zeros((1,nout))\r\n",
    "        self.dW = np.zeros_like(self.W)\r\n",
    "        self.db = np.zeros_like(self.b)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        self.x=x\r\n",
    "        return np.dot(x, self.W.T) + self.b\r\n",
    "    \r\n",
    "    def backward(self, dz):\r\n",
    "        dx = np.dot(dz, self.W)\r\n",
    "        dW = np.dot(dz.T, self.x)\r\n",
    "        db = dz.sum(axis=0)\r\n",
    "        self.dW = dW\r\n",
    "        self.db = db\r\n",
    "        return dx\r\n",
    "    \r\n",
    "    def update(self,lr):\r\n",
    "        self.W -= lr*self.dW\r\n",
    "        self.b -= lr*self.db"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class Softmax:\r\n",
    "    def forward(self,z):\r\n",
    "        self.z = z\r\n",
    "        zmax = z.max(axis=1,keepdims=True)\r\n",
    "        expz = np.exp(z-zmax)\r\n",
    "        Z = expz.sum(axis=1,keepdims=True)\r\n",
    "        return expz / Z\r\n",
    "    def backward(self,dp):\r\n",
    "        p = self.forward(self.z)\r\n",
    "        pdp = p * dp\r\n",
    "        return pdp - p * pdp.sum(axis=1, keepdims=True)\r\n",
    "    \r\n",
    "class CrossEntropyLoss:\r\n",
    "    def forward(self,p,y):\r\n",
    "        self.p = p\r\n",
    "        self.y = y\r\n",
    "        p_of_y = p[np.arange(len(y)), y]\r\n",
    "        log_prob = np.log(p_of_y)\r\n",
    "        return -log_prob.mean()\r\n",
    "    def backward(self,loss):\r\n",
    "        dlog_softmax = np.zeros_like(self.p)\r\n",
    "        dlog_softmax[np.arange(len(self.y)), self.y] -= 1.0/len(self.y)\r\n",
    "        return dlog_softmax / self.p"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "#Y = 1 / 1+e -z\r\n",
    "class Sigmoid:\r\n",
    "    def forward(self,z):\r\n",
    "        z = 1 / (1 + np.exp(-z))\r\n",
    "        self.z = z\r\n",
    "        return z\r\n",
    "    def backward(self,dz):\r\n",
    "        return self.z*(1-self.z)*dz\r\n",
    "\r\n",
    "\r\n",
    "class Tanh:\r\n",
    "    def forward(self,x):\r\n",
    "        y = np.tanh(x)\r\n",
    "        self.y = y\r\n",
    "        return y\r\n",
    "    def backward(self,dy):\r\n",
    "        return (1.0-self.y**2)*dy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "class Net:\r\n",
    "    def __init__(self):\r\n",
    "        self.layers = []\r\n",
    "    \r\n",
    "    def add(self,l):\r\n",
    "        self.layers.append(l)\r\n",
    "        \r\n",
    "    def forward(self,x):\r\n",
    "        for l in self.layers:\r\n",
    "            x = l.forward(x)\r\n",
    "        return x\r\n",
    "    \r\n",
    "    def backward(self,z):\r\n",
    "        for l in self.layers[::-1]:\r\n",
    "            z = l.backward(z)\r\n",
    "        return z\r\n",
    "    \r\n",
    "    def update(self,lr):\r\n",
    "        for l in self.layers:\r\n",
    "            if 'update' in l.__dir__():\r\n",
    "                l.update(lr)\r\n",
    "\r\n",
    "    def train_epoch(self, train_x, train_labels, loss=CrossEntropyLoss(), batch_size=4, lr=1e-5):\r\n",
    "        for i in range(0,len(train_x),batch_size):\r\n",
    "            xb = train_x[i:i+batch_size]\r\n",
    "            yb = train_labels[i:i+batch_size]\r\n",
    "\r\n",
    "            p = self.forward(xb)\r\n",
    "            l = loss.forward(p,yb)\r\n",
    "            dp = loss.backward(l)\r\n",
    "            dx = self.backward(dp)\r\n",
    "            net.update(lr)\r\n",
    "\r\n",
    "\r\n",
    "    def get_loss_acc(self,x,y,loss=CrossEntropyLoss()):\r\n",
    "        p = net.forward(x)\r\n",
    "        l = loss.forward(p,y)\r\n",
    "        pred = np.argmax(p,axis=1)\r\n",
    "        acc = (pred==y).mean()\r\n",
    "        return l,acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "source": [
    "net = Net()\r\n",
    "net.add(Linear(784,400))\r\n",
    "net.add(Tanh())\r\n",
    "net.add(Linear(400,10))\r\n",
    "net.add(Softmax())\r\n",
    "loss = CrossEntropyLoss()\r\n",
    "\r\n",
    "print(\"Initial loss={}, accuracy={}: \".format(*net.get_loss_acc(data_train,labels_train)))\r\n",
    "\r\n",
    "net.train_epoch(data_train,labels_train, loss=CrossEntropyLoss(), batch_size=4, lr=1e-3)\r\n",
    "        \r\n",
    "print(\"Final loss={}, accuracy={}: \".format(*net.get_loss_acc(data_train,labels_train)))\r\n",
    "print(\"Test loss={}, accuracy={}: \".format(*net.get_loss_acc(data_test,labels_test)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial loss=2.5940558928416704, accuracy=0.12391613361762616: \n",
      "Final loss=0.3913572785320046, accuracy=0.8918265813788202: \n",
      "Test loss=0.3994715421631222, accuracy=0.8891774891774892: \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "net = Net()\r\n",
    "net.add(Linear(784,400))\r\n",
    "net.add(Sigmoid())\r\n",
    "net.add(Linear(400,10))\r\n",
    "net.add(Softmax())\r\n",
    "loss = CrossEntropyLoss()\r\n",
    "\r\n",
    "print(\"Initial loss={}, accuracy={}: \".format(*net.get_loss_acc(data_train,labels_train)))\r\n",
    "\r\n",
    "net.train_epoch(data_train,labels_train, loss=CrossEntropyLoss(), batch_size=4, lr=2e-3)\r\n",
    "        \r\n",
    "print(\"Final loss={}, accuracy={}: \".format(*net.get_loss_acc(data_train,labels_train)))\r\n",
    "print(\"Test loss={}, accuracy={}: \".format(*net.get_loss_acc(data_test,labels_test)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial loss=2.605618184471634, accuracy=0.08407960199004975: \n",
      "Final loss=0.36651972726620624, accuracy=0.9038024164889836: \n",
      "Test loss=0.37828628913391, accuracy=0.8999278499278499: \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "source": [
    "net = Net()\r\n",
    "net.add(Linear(784,300))\r\n",
    "net.add(Sigmoid())\r\n",
    "net.add(Linear(300,50))\r\n",
    "net.add(Sigmoid())\r\n",
    "net.add(Linear(50,10))\r\n",
    "net.add(Softmax())\r\n",
    "loss = CrossEntropyLoss()\r\n",
    "\r\n",
    "print(\"Initial loss={}, accuracy={}: \".format(*net.get_loss_acc(data_train,labels_train)))\r\n",
    "\r\n",
    "net.train_epoch(data_train,labels_train, loss=CrossEntropyLoss(), batch_size=4, lr=2e-3)\r\n",
    "        \r\n",
    "print(\"Final loss={}, accuracy={}: \".format(*net.get_loss_acc(data_train,labels_train)))\r\n",
    "print(\"Test loss={}, accuracy={}: \".format(*net.get_loss_acc(data_test,labels_test)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial loss=2.3729930040918283, accuracy=0.11560056858564322: \n",
      "Final loss=0.8697533388151167, accuracy=0.8501776830135039: \n",
      "Test loss=0.8763696958277388, accuracy=0.850937950937951: \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "source": [
    "def load_mnist(path, kind='train'):\r\n",
    "    import os\r\n",
    "    import gzip\r\n",
    "    import numpy as np\r\n",
    "\r\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\r\n",
    "    labels_path = os.path.join(path,\r\n",
    "                               '%s-labels-idx1-ubyte.gz'\r\n",
    "                               % kind)\r\n",
    "    images_path = os.path.join(path,\r\n",
    "                               '%s-images-idx3-ubyte.gz'\r\n",
    "                               % kind)\r\n",
    "\r\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\r\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\r\n",
    "                               offset=8)\r\n",
    "\r\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\r\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\r\n",
    "                               offset=16).reshape(len(labels), 784)\r\n",
    "\r\n",
    "    return images, labels\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "source": [
    "X_train, y_train = load_mnist('', kind='train')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "source": [
    "X_train.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "metadata": {},
     "execution_count": 166
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "source": [
    "data_fashion_train, data_fashion_test, labels_fashion_train, labels_fashion_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "source": [
    "net = Net()\r\n",
    "net.add(Linear(784,400))\r\n",
    "net.add(Sigmoid())\r\n",
    "net.add(Linear(400,10))\r\n",
    "net.add(Softmax())\r\n",
    "loss = CrossEntropyLoss()\r\n",
    "\r\n",
    "print(\"Initial loss={}, accuracy={}: \".format(*net.get_loss_acc(data_fashion_train,labels_fashion_train)))\r\n",
    "\r\n",
    "net.train_epoch(data_fashion_train,labels_fashion_train, loss=CrossEntropyLoss(), batch_size=4, lr=1e-4)\r\n",
    "        \r\n",
    "print(\"Final loss={}, accuracy={}: \".format(*net.get_loss_acc(data_fashion_train,labels_fashion_train)))\r\n",
    "print(\"Test loss={}, accuracy={}: \".format(*net.get_loss_acc(data_fashion_test,labels_fashion_test)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial loss=2.3756211549568684, accuracy=0.12343283582089552: \n",
      "Final loss=0.9085345290498681, accuracy=0.7518407960199005: \n",
      "Test loss=0.9139401958875485, accuracy=0.7476767676767677: \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "net = Net()\r\n",
    "net.add(Linear(784,400))\r\n",
    "net.add(Sigmoid())\r\n",
    "net.add(Linear(400,100))\r\n",
    "net.add(Sigmoid())\r\n",
    "net.add(Linear(100,10))\r\n",
    "net.add(Softmax())\r\n",
    "loss = CrossEntropyLoss()\r\n",
    "\r\n",
    "print(\"Initial loss={}, accuracy={}: \".format(*net.get_loss_acc(data_fashion_train,labels_fashion_train)))\r\n",
    "\r\n",
    "net.train_epoch(data_fashion_train,labels_fashion_train, loss=CrossEntropyLoss(), batch_size=4, lr=1e-6)\r\n",
    "        \r\n",
    "print(\"Final loss={}, accuracy={}: \".format(*net.get_loss_acc(data_fashion_train,labels_fashion_train)))\r\n",
    "print(\"Test loss={}, accuracy={}: \".format(*net.get_loss_acc(data_fashion_test,labels_fashion_test)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial loss=2.394045004628358, accuracy=0.07074626865671642: \n",
      "Final loss=2.383507142703886, accuracy=0.07370646766169155: \n",
      "Test loss=2.3826876993071107, accuracy=0.07090909090909091: \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}